{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a44ebb6",
   "metadata": {},
   "source": [
    "# Concordancia espacializada\n",
    "\n",
    "> Notebook organizado para reprodutibilidade. Edite apenas a célula **CONFIGURAÇÕES**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# CONFIGURAÇÕES (edite se necessário)\n",
    "# A pasta raiz do projeto (por padrão, a pasta acima de /notebooks)\n",
    "ROOT = Path(os.getenv('CLIMBRA_PROJECT_ROOT', Path.cwd().parent)).resolve()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "RAW_DIR  = DATA_DIR / '00_raw'\n",
    "INT_DIR  = DATA_DIR / '01_intermediate'\n",
    "FINAL_DIR= DATA_DIR / '02_final'\n",
    "OUT_DIR  = ROOT / 'outputs'\n",
    "FIG_DIR  = OUT_DIR / 'figures'\n",
    "TAB_DIR  = OUT_DIR / 'tables'\n",
    "\n",
    "for d in [RAW_DIR, INT_DIR, FINAL_DIR, FIG_DIR, TAB_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3123b4-c106-4558-8f98-bf3fa05cc5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 01_calcular_concordancia_minibacias_SSP585.py\n",
    "# ============================================================\n",
    "# Objetivo\n",
    "#   Ler arquivos de vazão diária (1 arquivo por modelo; 926 colunas = minibacias),\n",
    "#   calcular (por minibacia e por horizonte):\n",
    "#     A) TENDÊNCIA (para MK e Theil-Sen):\n",
    "#        - Construir a série ANUAL como MÉDIA diária do ano (média anual).\n",
    "#        - Aplicar Mann-Kendall modificado (Hamed-Rao) e inclinação de Theil-Sen\n",
    "#          sobre essa série anual (por horizonte).\n",
    "#\n",
    "#     B) MAGNITUDE ROBUSTA (para ΔQ/Q):\n",
    "#        - Calcular o \"nível\" do período base como MEDIANA das vazões anuais\n",
    "#          no período 1980–2023 (por minibacia).\n",
    "#        - Calcular o \"nível\" do horizonte como MEDIANA das vazões anuais\n",
    "#          dentro do horizonte (por minibacia).\n",
    "#        - ΔQ/Q (%) = 100 * (Q_h_mediana - Q_base_mediana) / Q_base_mediana\n",
    "#        (A mediana aqui reduz a influência de anos extremos.)\n",
    "#\n",
    "#   Critério de significância por modelo e minibacia:\n",
    "#     (i) p-valor < 0,05 (MK modificado)\n",
    "#     (ii) |ΔQ/Q| > 10% (magnitudes robustas por mediana)\n",
    "#     (iii) sinal (aumento/redução) pelo sinal da inclinação de Theil-Sen\n",
    "#\n",
    "#   Além disso, calcula estatísticas do ENSEMBLE (média dos 19 modelos):\n",
    "#     - Série anual do ensemble = média das séries anuais de cada modelo\n",
    "#     - Tendência (MK + Sen) nessa série anual média (por horizonte)\n",
    "#     - ΔQ/Q robusto do ensemble (mediana anual do horizonte vs base)\n",
    "#\n",
    "# Saída\n",
    "#   CSV (formato \"long\"): sub_bacia × periodo + métricas (português),\n",
    "#   adequado para JOIN com shapefile e geração de mapas em 4 painéis.\n",
    "#\n",
    "# Entradas:\n",
    "#   FUTURO (2015-2100): E:\\RESULTADOS\\SSP5_85\\QTudo_Fut_Cenário 1\n",
    "#   HISTÓRICO (1980-2023): E:\\RESULTADOS\\SSP5_85\\Qtudo_Pres\n",
    "#\n",
    "# Dependências:\n",
    "#   pip install pandas numpy pymannkendall scipy\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pymannkendall as mk\n",
    "from scipy.stats import theilslopes\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURAÇÕES (AJUSTE AQUI)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Pastas (SSP5-8.5)\n",
    "PASTA_FUTURO = Path(r\"E:\\RESULTADOS_AB2\\SSP2-45\\QTudo_Fut_Cenario 2\")\n",
    "PASTA_PRES   = Path(r\"E:\\RESULTADOS_AB2\\SSP2-45\\Qtudo_Pres\")\n",
    "\n",
    "# Saída\n",
    "PASTA_SAIDA  = Path(r\"E:\\RESULTADOS_AB2\\SSP2-45\\_concordancia\")\n",
    "PASTA_SAIDA.mkdir(parents=True, exist_ok=True)\n",
    "ARQ_SAIDA = PASTA_SAIDA / \"concordancia_minibacias_ssp245.csv\"\n",
    "\n",
    "# Arquivos\n",
    "GLOB_ARQUIVOS = \"*.txt\"   # ajuste se for \"*.TXT\"\n",
    "N_COLS_MINIBACIAS = 926\n",
    "\n",
    "# Datas completas\n",
    "DATA_INI_PRES = \"1980-01-01\"\n",
    "DATA_FIM_PRES = \"2023-12-31\"\n",
    "\n",
    "DATA_INI_FUT  = \"2015-01-01\"\n",
    "DATA_FIM_FUT  = \"2100-12-31\"\n",
    "\n",
    "# Horizontes (para 4 painéis)\n",
    "HORIZONTES = {\n",
    "    \"Curto\": (\"2015-01-01\", \"2040-12-31\"),\n",
    "    \"Médio\": (\"2041-01-01\", \"2070-12-31\"),\n",
    "    \"Longo\": (\"2071-01-01\", \"2100-12-31\"),\n",
    "    \"Total\": (\"2015-01-01\", \"2100-12-31\"),\n",
    "}\n",
    "\n",
    "# Critérios\n",
    "ALFA_PVALOR = 0.05\n",
    "LIMIAR_DELTA_PCT = 10.0\n",
    "\n",
    "# Limiares de consenso (mapa 3 cores)\n",
    "LIMIAR_SEM_CONSENSO = 50.0\n",
    "LIMIAR_FORTE = 66.0\n",
    "\n",
    "# Leitura\n",
    "SEP = r\"\\s+\"\n",
    "ENGINE_CSV = \"python\"  # tolerante; se arquivos \"limpos\", pode ser \"c\"\n",
    "DTYPE = \"float32\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FUNÇÕES AUXILIARES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def lista_arquivos(pasta: Path, glob_pat: str) -> list[Path]:\n",
    "    arqs = sorted(pasta.glob(glob_pat))\n",
    "    if not arqs:\n",
    "        raise FileNotFoundError(f\"Nenhum arquivo encontrado em: {pasta} com padrão {glob_pat}\")\n",
    "    return arqs\n",
    "\n",
    "def ler_q_diario(arquivo: Path, data_ini: str, data_fim: str, n_cols: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lê arquivo sem cabeçalho com n_cols colunas (minibacias) e linhas diárias.\n",
    "    Faz ajuste robusto caso exista 1 linha extra (ex.: linha em branco no fim,\n",
    "    ou arquivo começando 1 dia antes / terminando 1 dia depois).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(arquivo, header=None, sep=SEP, engine=ENGINE_CSV)\n",
    "\n",
    "    # Remove linhas totalmente vazias (ex.: linha em branco no final)\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    if df.shape[1] != n_cols:\n",
    "        raise ValueError(f\"{arquivo.name}: esperado {n_cols} colunas, veio {df.shape[1]}\")\n",
    "\n",
    "    idx = pd.date_range(start=data_ini, end=data_fim, freq=\"D\")\n",
    "    n_esp = len(idx)\n",
    "    n_obs = df.shape[0]\n",
    "\n",
    "    if n_obs != n_esp:\n",
    "        # Caso típico: 1 linha extra\n",
    "        if n_obs == n_esp + 1:\n",
    "            # Heurística: se a primeira linha parece \"estranha\" (ex.: zeros ou muito diferente),\n",
    "            # pode ser dia extra no início. Sem datas no arquivo, o mais confiável é:\n",
    "            # tentar cortar início OU fim e seguir.\n",
    "            df_tail = df.iloc[1:].reset_index(drop=True)   # remove 1ª linha\n",
    "            df_head = df.iloc[:-1].reset_index(drop=True)  # remove última linha\n",
    "\n",
    "            # Escolha padrão: remover última linha (mais comum ser newline/linha extra)\n",
    "            # Mas se você suspeita que o arquivo começa em 2014-12-31, troque para df_tail.\n",
    "            df = df_head\n",
    "            n_obs = df.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{arquivo.name}: esperado {n_esp} linhas (dias), veio {n_obs}. \"\n",
    "                \"Verifique se há cabeçalho, linhas extras ou datas ausentes.\"\n",
    "            )\n",
    "\n",
    "    df.index = idx\n",
    "    df.columns = [f\"minibacia_{i+1:04d}\" for i in range(n_cols)]\n",
    "    return df.astype(DTYPE)\n",
    "\n",
    "def diario_para_serie_anual_media(df_diario: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Série ANUAL para tendência:\n",
    "    - Média diária do ano (média anual).\n",
    "    \"\"\"\n",
    "    return df_diario.resample(\"YS\").mean()\n",
    "\n",
    "def mk_pvalor_hamed_rao(x: np.ndarray) -> float:\n",
    "    if len(x) < 8:\n",
    "        return np.nan\n",
    "    return float(mk.hamed_rao_modification_test(x).p)\n",
    "\n",
    "def theil_sen_inclinacao(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Inclinação Theil-Sen por passo (aqui: anual).\n",
    "    \"\"\"\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    slope, _, _, _ = theilslopes(x, np.arange(len(x)))\n",
    "    return float(slope)\n",
    "\n",
    "def sinal_tendencia(slope: float) -> int:\n",
    "    if not np.isfinite(slope) or slope == 0:\n",
    "        return 0\n",
    "    return 1 if slope > 0 else -1\n",
    "\n",
    "def aplicar_criterios(delta_pct: float, pvalor: float, slope: float) -> int:\n",
    "    \"\"\"\n",
    "    Aplica os 3 critérios:\n",
    "      p < 0.05, |Δ| > 10%, sinal pelo slope\n",
    "    Retorna: +1 (aumento), -1 (redução), 0 (não significativo)\n",
    "    \"\"\"\n",
    "    if not (np.isfinite(delta_pct) and np.isfinite(pvalor) and np.isfinite(slope)):\n",
    "        return 0\n",
    "    if pvalor >= ALFA_PVALOR:\n",
    "        return 0\n",
    "    if abs(delta_pct) <= LIMIAR_DELTA_PCT:\n",
    "        return 0\n",
    "    return sinal_tendencia(slope)\n",
    "\n",
    "def classificar_consenso(agree_pct: float) -> str:\n",
    "    if not np.isfinite(agree_pct):\n",
    "        return \"sem_consenso\"\n",
    "    if agree_pct < LIMIAR_SEM_CONSENSO:\n",
    "        return \"sem_consenso\"\n",
    "    if agree_pct < LIMIAR_FORTE:\n",
    "        return \"consenso_moderado\"\n",
    "    return \"consenso_forte\"\n",
    "\n",
    "def classe_sinal_texto(n_pos: int, n_neg: int, agree_pct: float) -> str:\n",
    "    if (n_pos + n_neg) == 0:\n",
    "        return \"neutro sem consenso\"\n",
    "    if n_pos == n_neg:\n",
    "        return \"neutro sem consenso\"\n",
    "    dom = \"aumento\" if n_pos > n_neg else \"redução\"\n",
    "    cls = classificar_consenso(agree_pct)\n",
    "    if cls == \"sem_consenso\":\n",
    "        return f\"{dom} sem consenso\"\n",
    "    return dom\n",
    "\n",
    "def tendencia_ensemble_texto(slope: float, pvalor: float) -> str:\n",
    "    if not (np.isfinite(slope) and np.isfinite(pvalor)):\n",
    "        return \"sem tendência\"\n",
    "    if pvalor >= ALFA_PVALOR or slope == 0:\n",
    "        return \"sem tendência\"\n",
    "    return \"tendência de aumento\" if slope > 0 else \"tendência de redução\"\n",
    "\n",
    "def sinal_texto(slope: float) -> str:\n",
    "    s = sinal_tendencia(slope)\n",
    "    if s > 0:\n",
    "        return \"positivo\"\n",
    "    if s < 0:\n",
    "        return \"negativo\"\n",
    "    return \"neutro\"\n",
    "\n",
    "def mediana_anual_periodo(df_anual: pd.DataFrame, y0: int, y1: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Retorna a MEDIANA das vazões anuais (média anual) dentro do período [y0, y1],\n",
    "    por minibacia.\n",
    "    \"\"\"\n",
    "    sub = df_anual[(df_anual.index.year >= y0) & (df_anual.index.year <= y1)]\n",
    "    return sub.median(axis=0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PIPELINE PRINCIPAL\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def main() -> None:\n",
    "    # 1) Listar arquivos\n",
    "    arqs_fut = lista_arquivos(PASTA_FUTURO, GLOB_ARQUIVOS)\n",
    "    arqs_pres = lista_arquivos(PASTA_PRES, GLOB_ARQUIVOS)\n",
    "\n",
    "    # 2) HISTÓRICO: construir baseline robusto por mediana anual (1980–2023)\n",
    "    #    Etapas:\n",
    "    #      - diário -> série anual média (para consistência com tendência)\n",
    "    #      - nível do período base = mediana das vazões anuais\n",
    "    #      - baseline multimodelo (se houver vários arquivos) = média dos baselines por modelo\n",
    "    print(f\"[INFO] Arquivos histórico (PRES): {len(arqs_pres)}\")\n",
    "    base_por_modelo = []\n",
    "\n",
    "    for fp in arqs_pres:\n",
    "        q_pres_d = ler_q_diario(fp, DATA_INI_PRES, DATA_FIM_PRES, N_COLS_MINIBACIAS)\n",
    "        q_pres_y = diario_para_serie_anual_media(q_pres_d)  # série anual (média)\n",
    "        q_base_med = q_pres_y.median(axis=0)                # nível robusto do período base (mediana dos anos)\n",
    "        base_por_modelo.append(q_base_med)\n",
    "\n",
    "    if len(base_por_modelo) == 1:\n",
    "        q_base = base_por_modelo[0]\n",
    "    else:\n",
    "        # baseline multimodelo: média entre modelos dos níveis base (robustos por mediana anual)\n",
    "        q_base = pd.concat(base_por_modelo, axis=1).mean(axis=1)\n",
    "\n",
    "    minibacias = q_base.index.tolist()\n",
    "    n_minis = len(minibacias)\n",
    "\n",
    "    # 3) Preparar acumuladores para concordância e ensemble\n",
    "    anos_fut = pd.date_range(start=DATA_INI_FUT, end=DATA_FIM_FUT, freq=\"YS\")  # 2015..2100\n",
    "    n_anos = len(anos_fut)\n",
    "\n",
    "    # Ensemble anual: soma das séries anuais (média anual) dos modelos, depois divide por N\n",
    "    ensemble_anual_soma = np.zeros((n_anos, n_minis), dtype=np.float64)\n",
    "\n",
    "    contagens = {\n",
    "        nome: {\n",
    "            \"n_pos\": np.zeros(n_minis, dtype=np.int32),\n",
    "            \"n_neg\": np.zeros(n_minis, dtype=np.int32),\n",
    "        }\n",
    "        for nome in HORIZONTES.keys()\n",
    "    }\n",
    "\n",
    "    # 4) FUTURO: loop por modelo\n",
    "    print(f\"[INFO] Arquivos futuro (FUT): {len(arqs_fut)}\")\n",
    "    for fp in arqs_fut:\n",
    "        print(f\"[INFO] Processando modelo: {fp.name}\")\n",
    "\n",
    "        q_fut_d = ler_q_diario(fp, DATA_INI_FUT, DATA_FIM_FUT, N_COLS_MINIBACIAS)\n",
    "        q_fut_y = diario_para_serie_anual_media(q_fut_d)  # série anual (média)\n",
    "\n",
    "        # alinhar eixo anual\n",
    "        q_fut_y = q_fut_y.reindex(anos_fut)\n",
    "        if q_fut_y.isna().any().any():\n",
    "            raise ValueError(f\"Há NaNs após reindex anual para {fp.name}. Verifique integridade das datas.\")\n",
    "\n",
    "        # acumula ensemble anual\n",
    "        ensemble_anual_soma += q_fut_y.to_numpy(dtype=np.float64)\n",
    "\n",
    "        # Avaliação por horizonte\n",
    "        for nome_h, (ini, fim) in HORIZONTES.items():\n",
    "            y0 = pd.to_datetime(ini).year\n",
    "            y1 = pd.to_datetime(fim).year\n",
    "\n",
    "            h = q_fut_y[(q_fut_y.index.year >= y0) & (q_fut_y.index.year <= y1)]\n",
    "            if len(h) < 8:\n",
    "                continue\n",
    "\n",
    "            # ΔQ/Q robusto: mediana anual do horizonte vs mediana anual do base\n",
    "            q_h_med = h.median(axis=0)  # Series (mediana dos anos do horizonte)\n",
    "            delta_pct = 100.0 * (q_h_med - q_base) / q_base\n",
    "\n",
    "            # tendência: MK + Sen na série anual (média anual)\n",
    "            h_np = h.to_numpy(dtype=np.float64)\n",
    "            for j in range(n_minis):\n",
    "                x = h_np[:, j]\n",
    "                if not np.all(np.isfinite(x)):\n",
    "                    continue\n",
    "\n",
    "                p = mk_pvalor_hamed_rao(x)\n",
    "                slope = theil_sen_inclinacao(x)\n",
    "                sgn = aplicar_criterios(float(delta_pct.iloc[j]), p, slope)\n",
    "\n",
    "                if sgn == 1:\n",
    "                    contagens[nome_h][\"n_pos\"][j] += 1\n",
    "                elif sgn == -1:\n",
    "                    contagens[nome_h][\"n_neg\"][j] += 1\n",
    "\n",
    "    # 5) Ensemble anual final\n",
    "    n_modelos_fut = len(arqs_fut)\n",
    "    if n_modelos_fut == 0:\n",
    "        raise RuntimeError(\"Nenhum arquivo futuro encontrado.\")\n",
    "    ensemble_anual = ensemble_anual_soma / float(n_modelos_fut)\n",
    "\n",
    "    # 6) Montar tabela final (minibacia × período)\n",
    "    linhas = []\n",
    "\n",
    "    for nome_h, (ini, fim) in HORIZONTES.items():\n",
    "        n_pos = contagens[nome_h][\"n_pos\"]\n",
    "        n_neg = contagens[nome_h][\"n_neg\"]\n",
    "\n",
    "        agree_n = np.maximum(n_pos, n_neg)\n",
    "        agree_pct = 100.0 * agree_n / float(n_modelos_fut)\n",
    "\n",
    "        # Recorte do ensemble no horizonte\n",
    "        y0 = pd.to_datetime(ini).year\n",
    "        y1 = pd.to_datetime(fim).year\n",
    "        mask_ens = (anos_fut.year >= y0) & (anos_fut.year <= y1)\n",
    "        ens_h = ensemble_anual[mask_ens, :]\n",
    "\n",
    "        # ΔQ/Q robusto do ensemble: mediana anual do horizonte vs base (q_base)\n",
    "        ens_h_med = np.nanmedian(ens_h, axis=0)\n",
    "        ens_delta_pct = 100.0 * (ens_h_med - q_base.to_numpy(dtype=np.float64)) / q_base.to_numpy(dtype=np.float64)\n",
    "\n",
    "        # Tendência do ensemble: MK + Sen na série anual do ensemble\n",
    "        for j, mini in enumerate(minibacias):\n",
    "            x = ens_h[:, j]\n",
    "            if not np.all(np.isfinite(x)) or len(x) < 8:\n",
    "                ens_p = np.nan\n",
    "                ens_slope = np.nan\n",
    "            else:\n",
    "                ens_p = mk_pvalor_hamed_rao(x)\n",
    "                ens_slope = theil_sen_inclinacao(x)\n",
    "\n",
    "            pct = float(agree_pct[j])\n",
    "            classe_consenso = classificar_consenso(pct)\n",
    "            texto_classe_sinal = classe_sinal_texto(int(n_pos[j]), int(n_neg[j]), pct)\n",
    "\n",
    "            ens_trend_txt = tendencia_ensemble_texto(ens_slope, ens_p)\n",
    "            ens_sinal_txt = sinal_texto(ens_slope)\n",
    "\n",
    "            linhas.append({\n",
    "                \"sub_bacia\": mini.replace(\"minibacia_\", \"\"),\n",
    "                \"periodo\": nome_h,\n",
    "\n",
    "                \"modelos_total\": int(n_modelos_fut),\n",
    "                \"modelos_significativos\": int(n_pos[j] + n_neg[j]),\n",
    "                \"positivos\": int(n_pos[j]),\n",
    "                \"negativos\": int(n_neg[j]),\n",
    "                \"concordancia_pct\": round(pct, 1),\n",
    "\n",
    "                \"classe_consenso\": classe_consenso,\n",
    "                \"classe_sinal\": texto_classe_sinal,\n",
    "\n",
    "                \"ensemble_tendencia\": ens_trend_txt,\n",
    "                \"ensemble_p\": None if not np.isfinite(ens_p) else float(ens_p),\n",
    "                \"ensemble_slope\": None if not np.isfinite(ens_slope) else float(ens_slope),\n",
    "                \"ensemble_var_relativa_pct\": None if not np.isfinite(ens_delta_pct[j]) else float(ens_delta_pct[j]),\n",
    "                \"ensemble_significativo\": bool(\n",
    "                    np.isfinite(ens_p)\n",
    "                    and (ens_p < ALFA_PVALOR)\n",
    "                    and (abs(ens_delta_pct[j]) > LIMIAR_DELTA_PCT)\n",
    "                ),\n",
    "                \"ensemble_sinal\": ens_sinal_txt,\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(linhas)\n",
    "\n",
    "    # Ordenação\n",
    "    df_out[\"sub_bacia\"] = df_out[\"sub_bacia\"].astype(int)\n",
    "    ordem_periodos = pd.CategoricalDtype([\"Curto\", \"Médio\", \"Longo\", \"Total\"], ordered=True)\n",
    "    df_out[\"periodo\"] = df_out[\"periodo\"].astype(ordem_periodos)\n",
    "    df_out = df_out.sort_values([\"sub_bacia\", \"periodo\"]).reset_index(drop=True)\n",
    "\n",
    "    # Exporta\n",
    "    df_out.to_csv(ARQ_SAIDA, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] Tabela exportada em: {ARQ_SAIDA}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf5dfc-9e44-4db9-8fbc-ee14812ad76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script: gera_shapefiles_concordancia_e_mapas_4paineis.py\n",
    "\n",
    "Descrição geral\n",
    "---------------\n",
    "Este script faz duas etapas principais, em sequência:\n",
    "\n",
    "1) Geração de shapefiles temáticos de CONCORDÂNCIA (classe de consenso)\n",
    "   - Lê um CSV com resultados (minibacia × período) contendo:\n",
    "       sub_bacia, periodo, concordancia_pct, classe_consenso, ...\n",
    "   - Cruza esses dados com o shapefile das minibacias (minis_mgb.shp).\n",
    "   - Para cada período (Curto/Médio/Longo/Total), grava um shapefile:\n",
    "       minis_concord_{periodo}.shp\n",
    "   - IMPORTANTE: Shapefile limita nomes de campos a 10 caracteres.\n",
    "     Por isso, o script renomeia colunas para nomes curtos antes de salvar.\n",
    "\n",
    "2) Geração de mapa 2×2 (4 painéis) para o cenário\n",
    "   - Lê os shapefiles gerados na etapa (1).\n",
    "   - Plota 4 painéis: Curto, Médio, Longo e Total\n",
    "   - Simbologia em 3 cores (sem_consenso / consenso_moderado / consenso_forte)\n",
    "   - Sobrepõe:\n",
    "       • contorno das sub-bacias (Subbacias.shp)\n",
    "       • pontos das cidades de Curitiba e União da Vitória\n",
    "       • rótulos com halo\n",
    "\n",
    "Uso esperado na pipeline\n",
    "------------------------\n",
    "1. Ajustar os CAMINHOS e o campo ID da minibacia (ID_FIELD).\n",
    "2. Executar o script.\n",
    "3. Usar a figura final (painéis 2×2) diretamente na dissertação.\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================\n",
    "# IMPORTAÇÕES\n",
    "# ===================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import patheffects\n",
    "from pathlib import Path\n",
    "\n",
    "# ===================================================\n",
    "# 0. CONFIGURAÇÕES – AJUSTAR AQUI\n",
    "# ===================================================\n",
    "\n",
    "# Pasta base do cenário\n",
    "base_dir = Path(r\"E:\\RESULTADOS_AB2\\SSP2-45\")  # AJUSTE SE NECESSÁRIO\n",
    "\n",
    "# CSV de concordância (gerado no script anterior)\n",
    "# Deve conter: sub_bacia, periodo, concordancia_pct, classe_consenso, ...\n",
    "csv_concord = base_dir / \"_concordancia\" / \"concordancia_minibacias_ssp245.csv\"\n",
    "\n",
    "# Shapefile das minibacias MGB\n",
    "shp_minis = Path(r\"E:\\IGUAÇU_OTTO\\6_Calibração\\minis_mgb.shp\")\n",
    "ID_FIELD  = \"ID_Mini\"  # campo de ID da minibacia no shapefile\n",
    "\n",
    "# Pasta onde serão salvos os shapefiles temáticos\n",
    "shapes_dir = base_dir / \"shapes_concordancia\"\n",
    "shapes_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Pasta onde serão salvos os mapas finais (4 painéis)\n",
    "fig_out_dir = base_dir / \"Mapas_4paineis_Concordancia_mediana\"\n",
    "fig_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Shapefile das sub-bacias (para contorno)\n",
    "shp_sub = Path(r\"E:\\IGUAÇU_OTTO\\Shp\\Subbacias.shp\")\n",
    "\n",
    "# Shapefile das cidades (contendo Curitiba e União da Vitória)\n",
    "shp_cidades = Path(\n",
    "    r\"G:\\Meu Drive\\2_MESTRADO\\1_Dissertação\\Figuras\\20250516_SHAPES_FIGURA\\GEOFT_CIDADE_2016.shp\"\n",
    ")\n",
    "CAMPO_NOME_CIDADE = \"CID_NM\"\n",
    "\n",
    "# Períodos (ordem fixa para painel)\n",
    "periodos_ordem = [\"Curto\", \"Médio\", \"Longo\", \"Total\"]\n",
    "\n",
    "# Cores (3 classes) – ajuste se desejar\n",
    "CORES_CONSENSO = {\n",
    "    \"sem_consenso\":       \"#d9d9d9\",  # cinza claro\n",
    "    \"consenso_moderado\":  \"#9ecae1\",  # azul claro\n",
    "    \"consenso_forte\":     \"#3182bd\",  # azul escuro\n",
    "}\n",
    "\n",
    "# ===================================================\n",
    "# 1. ETAPA 1 – GERAR SHAPEFILES TEMÁTICOS (CONCORDÂNCIA)\n",
    "# ===================================================\n",
    "\n",
    "print(\"\\n=== ETAPA 1: Gerando shapefiles temáticos de concordância por período ===\\n\")\n",
    "\n",
    "# 1.1 Ler CSV\n",
    "df = pd.read_csv(csv_concord)\n",
    "\n",
    "# Colunas obrigatórias\n",
    "req_cols = {\"sub_bacia\", \"periodo\", \"classe_consenso\", \"concordancia_pct\"}\n",
    "faltando = req_cols - set(df.columns)\n",
    "if faltando:\n",
    "    raise ValueError(f\"CSV não contém colunas obrigatórias: {faltando}\")\n",
    "\n",
    "# 1.2 Padronizar tipos\n",
    "df[\"sub_bacia\"] = df[\"sub_bacia\"].astype(int)\n",
    "df[\"periodo\"] = df[\"periodo\"].astype(str)\n",
    "\n",
    "print(\"Períodos encontrados no CSV:\", sorted(df[\"periodo\"].unique()))\n",
    "print(\"Minibacias no CSV:\", df[\"sub_bacia\"].nunique())\n",
    "\n",
    "# 1.3 Ler shapefile minibacias\n",
    "minis_gdf = gpd.read_file(shp_minis)\n",
    "minis_gdf[ID_FIELD] = minis_gdf[ID_FIELD].astype(int)\n",
    "\n",
    "# 1.4 Função para “limpar” o rótulo do período para nome de arquivo\n",
    "def periodo_para_nome(per: str) -> str:\n",
    "    return (\n",
    "        per.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"é\", \"e\")\n",
    "        .replace(\"í\", \"i\")\n",
    "        .replace(\"ó\", \"o\")\n",
    "        .replace(\"ã\", \"a\")\n",
    "        .replace(\"ç\", \"c\")\n",
    "    )\n",
    "\n",
    "# 1.5 Renomear colunas para compatibilidade com Shapefile (<= 10 caracteres)\n",
    "REN_SHAPE = {\n",
    "    \"concordancia_pct\": \"conc_pct\",   # 8\n",
    "    \"classe_consenso\":  \"cls_cons\",   # 8\n",
    "    \"classe_sinal\":     \"cls_sinal\",  # 9\n",
    "    \"ensemble_tendencia\": \"ens_trend\",  # 9\n",
    "    \"ensemble_var_relativa_pct\": \"ens_var\",  # 7\n",
    "    \"ensemble_significativo\": \"ens_sig\",     # 7\n",
    "    \"ensemble_sinal\": \"ens_sinal\",           # 9\n",
    "    \"ensemble_slope\": \"ens_slope\",           # 9\n",
    "    \"ensemble_p\": \"ens_p\",                   # 5\n",
    "}\n",
    "\n",
    "# 1.6 Gerar shapefile por período\n",
    "for per in periodos_ordem:\n",
    "    df_p = df[df[\"periodo\"] == per].copy()\n",
    "\n",
    "    # Seleciona colunas úteis (se existirem)\n",
    "    cols_base = [\"sub_bacia\", \"concordancia_pct\", \"classe_consenso\"]\n",
    "    cols_opc = [\n",
    "        \"classe_sinal\",\n",
    "        \"ensemble_tendencia\",\n",
    "        \"ensemble_p\",\n",
    "        \"ensemble_slope\",\n",
    "        \"ensemble_var_relativa_pct\",\n",
    "        \"ensemble_significativo\",\n",
    "        \"ensemble_sinal\",\n",
    "    ]\n",
    "    cols = cols_base + [c for c in cols_opc if c in df_p.columns]\n",
    "    df_p = df_p[cols].copy()\n",
    "\n",
    "    # Merge com minibacias\n",
    "    gdf_p = minis_gdf.merge(df_p, left_on=ID_FIELD, right_on=\"sub_bacia\", how=\"left\")\n",
    "\n",
    "    # Renomeia colunas para o Shapefile (limite 10 caracteres)\n",
    "    gdf_p = gdf_p.rename(columns={k: v for k, v in REN_SHAPE.items() if k in gdf_p.columns})\n",
    "\n",
    "    # Salvar\n",
    "    per_limpo = periodo_para_nome(per)\n",
    "    shp_out = shapes_dir / f\"minis_concord_{per_limpo}.shp\"\n",
    "    gdf_p.to_file(shp_out)\n",
    "    print(f\"Shapefile salvo: {shp_out.name}\")\n",
    "\n",
    "print(\"\\n✔ ETAPA 1 concluída: shapefiles gerados em:\", shapes_dir)\n",
    "\n",
    "# ===================================================\n",
    "# 2. ETAPA 2 – GERAR MAPA 2×2 (4 PAINÉIS)\n",
    "# ===================================================\n",
    "\n",
    "print(\"\\n=== ETAPA 2: Gerando mapa 4 painéis (Curto/Médio/Longo/Total) ===\\n\")\n",
    "\n",
    "# Campo de classe no shapefile (renomeado)\n",
    "CAMPO_CLASSE = \"cls_cons\"\n",
    "\n",
    "# 2.1 Ler shapefiles gerados\n",
    "shps = {}\n",
    "for per in periodos_ordem:\n",
    "    per_limpo = periodo_para_nome(per)\n",
    "    shp_path = shapes_dir / f\"minis_concord_{per_limpo}.shp\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"Não encontrei shapefile do período {per}: {shp_path}\")\n",
    "    shps[per] = shp_path\n",
    "\n",
    "gdfs = {per: gpd.read_file(path) for per, path in shps.items()}\n",
    "\n",
    "# Verificação: garantir que CAMPO_CLASSE existe\n",
    "cols_check = list(gdfs[periodos_ordem[0]].columns)\n",
    "if CAMPO_CLASSE not in cols_check:\n",
    "    raise KeyError(\n",
    "        f\"Campo '{CAMPO_CLASSE}' não encontrado no shapefile. Colunas disponíveis: {cols_check}\\n\"\n",
    "        \"Verifique REN_SHAPE e/ou se o Shapefile truncou nomes de forma diferente.\"\n",
    "    )\n",
    "\n",
    "# CRS\n",
    "crs_minis = list(gdfs.values())[0].crs\n",
    "\n",
    "# 2.2 Sub-bacias e cidades\n",
    "gdf_sub = gpd.read_file(shp_sub).to_crs(crs_minis)\n",
    "gdf_cid = gpd.read_file(shp_cidades).to_crs(crs_minis)\n",
    "\n",
    "mask_cur = gdf_cid[CAMPO_NOME_CIDADE].str.contains(r\"^curitiba$\", case=False, na=False, regex=True)\n",
    "mask_un  = gdf_cid[CAMPO_NOME_CIDADE].str.contains(\"uni[aã]o da vit\", case=False, na=False, regex=True)\n",
    "cidades_sel = gdf_cid[mask_cur | mask_un].copy()\n",
    "\n",
    "# Bounds (usar do primeiro)\n",
    "xmin, ymin, xmax, ymax = list(gdfs.values())[0].total_bounds\n",
    "dx = (xmax - xmin) * 0.01\n",
    "dy = (ymax - ymin) * 0.01\n",
    "\n",
    "# 2.3 Figura 2×2 + legenda\n",
    "fig = plt.figure(figsize=(14, 10), dpi=300)\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.25], wspace=0.02, hspace=0.05)\n",
    "\n",
    "ax_11 = fig.add_subplot(gs[0, 0])\n",
    "ax_12 = fig.add_subplot(gs[0, 1])\n",
    "ax_21 = fig.add_subplot(gs[1, 0])\n",
    "ax_22 = fig.add_subplot(gs[1, 1])\n",
    "ax_leg = fig.add_subplot(gs[:, 2])\n",
    "\n",
    "axes = [ax_11, ax_12, ax_21, ax_22]\n",
    "# Ordem lógica (continua igual)\n",
    "painel_ordem = [\"Curto\", \"Médio\", \"Longo\", \"Total\"]\n",
    "\n",
    "# Rótulos de exibição\n",
    "rotulos_paineis = {\n",
    "    \"Curto\": \"Horizonte 2015–2040\",\n",
    "    \"Médio\": \"Horizonte 2041–2070\",\n",
    "    \"Longo\": \"Horizonte 2071–2100\",\n",
    "    \"Total\": \"Horizonte Total (2015–2100)\",\n",
    "}\n",
    "\n",
    "for ax, per in zip(axes, painel_ordem):\n",
    "    gdf = gdfs[per]\n",
    "\n",
    "    # Plot por categoria (3 cores fixas)\n",
    "    for classe, cor in CORES_CONSENSO.items():\n",
    "        sel = gdf[gdf[CAMPO_CLASSE] == classe]\n",
    "        if len(sel) == 0:\n",
    "            continue\n",
    "        sel.plot(ax=ax, color=cor, edgecolor=\"black\", linewidth=0.05)\n",
    "\n",
    "    # Contorno sub-bacias\n",
    "    gdf_sub.boundary.plot(ax=ax, edgecolor=\"grey\", linewidth=1.2, zorder=3)\n",
    "\n",
    "    # Cidades + rótulos\n",
    "    if not cidades_sel.empty:\n",
    "        cidades_sel.plot(ax=ax, marker=\"^\", color=\"black\", markersize=35, zorder=4, linewidth=0)\n",
    "\n",
    "        for _, row in cidades_sel.iterrows():\n",
    "            x = row.geometry.x\n",
    "            y = row.geometry.y\n",
    "            nome = row[CAMPO_NOME_CIDADE]\n",
    "\n",
    "            txt = ax.text(\n",
    "                x + dx, y + dy, nome,\n",
    "                fontsize=8.5, color=\"white\",\n",
    "                ha=\"left\", va=\"bottom\", zorder=5\n",
    "            )\n",
    "            txt.set_path_effects([\n",
    "                patheffects.Stroke(linewidth=1.4, foreground=\"black\"),\n",
    "                patheffects.Normal()\n",
    "            ])\n",
    "\n",
    "    ax.set_title(rotulos_paineis[per], fontsize=11)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# 2.4 Legenda customizada (3 classes)\n",
    "# 2.4 Legenda customizada (3 classes) – CONCORDÂNCIA\n",
    "ax_leg.axis(\"off\")\n",
    "\n",
    "ITENS_LEGENDA = [\n",
    "    (\"sem_consenso\", \"Sem consenso (<50%)\"),\n",
    "    (\"consenso_moderado\", \"Consenso moderado (50–66%)\"),\n",
    "    (\"consenso_forte\", \"Consenso forte (>66%)\"),\n",
    "]\n",
    "\n",
    "# Parâmetros de layout (ajuste fino aqui se quiser)\n",
    "n = len(ITENS_LEGENDA)\n",
    "dy_leg = 0.08          # espaçamento vertical entre itens\n",
    "y_center = 0.45        # centro vertical do bloco de itens (0 a 1)\n",
    "\n",
    "# Centro do bloco de itens\n",
    "y0 = y_center + (n - 1) * dy_leg / 2\n",
    "\n",
    "# Título da legenda (centralizado em relação ao bloco)\n",
    "ax_leg.text(\n",
    "    0.5, y0 + 0.08,\n",
    "    \"Grau de concordância\",\n",
    "    transform=ax_leg.transAxes,\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\"\n",
    ")\n",
    "\n",
    "# Itens (caixa + texto)\n",
    "for i, (key, label) in enumerate(ITENS_LEGENDA):\n",
    "    y = y0 - i * dy_leg\n",
    "\n",
    "    # Caixa de cor\n",
    "    ax_leg.add_patch(\n",
    "        plt.Rectangle(\n",
    "            (0.14, y - 0.03), 0.12, 0.06,\n",
    "            color=CORES_CONSENSO[key],\n",
    "            transform=ax_leg.transAxes,\n",
    "            ec=\"black\",\n",
    "            lw=0.4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Texto\n",
    "    ax_leg.text(\n",
    "        0.30, y,\n",
    "        label,\n",
    "        transform=ax_leg.transAxes,\n",
    "        fontsize=10,\n",
    "        va=\"center\",\n",
    "        ha=\"left\"\n",
    "    )\n",
    "\n",
    "# Título geral\n",
    "fig.suptitle(\n",
    "    \"Grau de concordância entre modelos – Vazão média anual (SSP2-4.5)\",\n",
    "    fontsize=14, weight=\"bold\", y=0.98\n",
    ")\n",
    "\n",
    "# Salvar\n",
    "fig.tight_layout(rect=[0.03, 0.03, 0.98, 0.95])\n",
    "fig_path = fig_out_dir / \"MAPA_4PAINEIS_CONCORDANCIA_SSP245.png\"\n",
    "fig.savefig(fig_path, dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\n✔ Figura salva: {fig_path}\")\n",
    "print(\"\\n✨ ETAPA 2 FINALIZADA – Mapa 4 painéis gerado com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810004d0-643f-4da3-ba46-02da0771f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script: gera_shapefiles_ensemble_e_mapas_4paineis.py\n",
    "\n",
    "Objetivo\n",
    "--------\n",
    "Gerar mapas 2×2 (4 painéis: Curto/Médio/Longo/Total) mostrando APENAS a\n",
    "significância do ENSEMBLE (média dos modelos) por minibacia, com base em 3 critérios:\n",
    "\n",
    "  (i)  ensemble_p < 0,05   (Mann-Kendall modificado aplicado à série anual do ensemble)\n",
    "  (ii) |ensemble_var_relativa_pct| > 10%\n",
    "  (iii) sinal da tendência via ensemble_slope (Theil-Sen)\n",
    "\n",
    "Classificação (3 classes)\n",
    "-------------------------\n",
    "- \"sem_significancia\"       (não atende aos 3 critérios)\n",
    "- \"aumento_significativo\"   (atende e slope > 0)\n",
    "- \"reducao_significativa\"   (atende e slope < 0)\n",
    "\n",
    "Entradas\n",
    "--------\n",
    "- CSV de concordância já calculado (contém colunas do ensemble):\n",
    "    sub_bacia, periodo, ensemble_p, ensemble_slope, ensemble_var_relativa_pct\n",
    "- Shapefile das minibacias (minis_mgb.shp)\n",
    "- Shapefile das sub-bacias (contorno)\n",
    "- Shapefile das cidades (Curitiba e União da Vitória)\n",
    "\n",
    "Saídas\n",
    "------\n",
    "- Shapefiles temáticos por período (4):\n",
    "    minis_enssig_curto.shp, minis_enssig_medio.shp, minis_enssig_longo.shp, minis_enssig_total.shp\n",
    "- Figura final 2×2:\n",
    "    MAPA_4PAINEIS_ENSEMBLE_SIGNIFICANCIA_SSP245.png\n",
    "\n",
    "Observação importante (Shapefile)\n",
    "---------------------------------\n",
    "Shapefile limita nomes de campos a 10 caracteres. O script renomeia colunas\n",
    "antes de salvar (ex.: \"ensemble_p\" -> \"ens_p\", \"classe_ensemble\" -> \"cls_ens\").\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================\n",
    "# IMPORTAÇÕES\n",
    "# ===================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import patheffects\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ===================================================\n",
    "# 0. CONFIGURAÇÕES – AJUSTAR AQUI\n",
    "# ===================================================\n",
    "\n",
    "# Pasta base do cenário\n",
    "base_dir = Path(r\"E:\\RESULTADOS_AB2\\SSP2-45\")  # AJUSTE SE NECESSÁRIO\n",
    "\n",
    "# CSV (gerado no cálculo anterior) – precisa ter colunas do ensemble\n",
    "csv_in = base_dir / \"_concordancia\" / \"concordancia_minibacias_ssp245.csv\"\n",
    "\n",
    "# Shapefile das minibacias\n",
    "shp_minis = Path(r\"E:\\IGUAÇU_OTTO\\6_Calibração\\minis_mgb.shp\")\n",
    "ID_FIELD  = \"ID_Mini\"  # campo de ID da minibacia no shapefile\n",
    "\n",
    "# Sub-bacias (contorno)\n",
    "shp_sub = Path(r\"E:\\IGUAÇU_OTTO\\Shp\\Subbacias.shp\")\n",
    "\n",
    "# Cidades (Curitiba e União da Vitória)\n",
    "shp_cidades = Path(\n",
    "    r\"G:\\Meu Drive\\2_MESTRADO\\1_Dissertação\\Figuras\\20250516_SHAPES_FIGURA\\GEOFT_CIDADE_2016.shp\"\n",
    ")\n",
    "CAMPO_NOME_CIDADE = \"CID_NM\"\n",
    "\n",
    "# Saídas\n",
    "shapes_dir = base_dir / \"shapes_ensemble_signif\"\n",
    "shapes_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fig_out_dir = base_dir / \"Mapas_4paineis_EnsembleSignif\"\n",
    "fig_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fig_path = fig_out_dir / \"MAPA_4PAINEIS_ENSEMBLE_SIGNIFICANCIA_SSP245.png\"\n",
    "\n",
    "# Períodos (ordem fixa)\n",
    "periodos_ordem = [\"Curto\", \"Médio\", \"Longo\", \"Total\"]\n",
    "\n",
    "# Critérios\n",
    "ALFA_P = 0.05\n",
    "LIMIAR_DELTA = 10.0  # |var_rel| > 10%\n",
    "\n",
    "# Cores (3 classes)\n",
    "CORES = {\n",
    "    \"sem_significancia\":      \"#d9d9d9\",  # cinza\n",
    "    \"aumento_significativo\":  \"#3182bd\",  # azul\n",
    "    \"reducao_significativa\":  \"#de2d26\",  # vermelho\n",
    "}\n",
    "\n",
    "# Legenda\n",
    "LEGENDA = [\n",
    "    (\"sem_significancia\", \"Sem significância\"),\n",
    "    (\"aumento_significativo\", \"Aumento significativo\"),\n",
    "    (\"reducao_significativa\", \"Redução significativa\"),\n",
    "]\n",
    "\n",
    "# Função para nomes de arquivo\n",
    "def periodo_para_nome(per: str) -> str:\n",
    "    return (\n",
    "        per.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"é\", \"e\")\n",
    "        .replace(\"í\", \"i\")\n",
    "        .replace(\"ó\", \"o\")\n",
    "        .replace(\"ã\", \"a\")\n",
    "        .replace(\"ç\", \"c\")\n",
    "    )\n",
    "\n",
    "# ===================================================\n",
    "# 1. ETAPA 1 – GERAR SHAPEFILES TEMÁTICOS (ENSEMBLE)\n",
    "# ===================================================\n",
    "\n",
    "print(\"\\n=== ETAPA 1: Gerando shapefiles temáticos de significância do ensemble ===\\n\")\n",
    "\n",
    "df = pd.read_csv(csv_in)\n",
    "\n",
    "# Verificações\n",
    "cols_req = {\"sub_bacia\", \"periodo\", \"ensemble_p\", \"ensemble_slope\", \"ensemble_var_relativa_pct\"}\n",
    "faltando = cols_req - set(df.columns)\n",
    "if faltando:\n",
    "    raise ValueError(\n",
    "        f\"CSV não contém colunas obrigatórias para o ensemble: {faltando}\\n\"\n",
    "        \"Verifique se o script de cálculo exportou as colunas do ensemble.\"\n",
    "    )\n",
    "\n",
    "# Padronizar tipos\n",
    "df[\"sub_bacia\"] = df[\"sub_bacia\"].astype(int)\n",
    "df[\"periodo\"] = df[\"periodo\"].astype(str)\n",
    "\n",
    "# Criar classe de significância do ensemble (3 classes)\n",
    "def classificar_ensemble(row) -> str:\n",
    "    p = row[\"ensemble_p\"]\n",
    "    slope = row[\"ensemble_slope\"]\n",
    "    varp = row[\"ensemble_var_relativa_pct\"]\n",
    "\n",
    "    # Trata NaN\n",
    "    if pd.isna(p) or pd.isna(slope) or pd.isna(varp):\n",
    "        return \"sem_significancia\"\n",
    "\n",
    "    atende = (p < ALFA_P) and (abs(varp) > LIMIAR_DELTA)\n",
    "\n",
    "    if not atende:\n",
    "        return \"sem_significancia\"\n",
    "\n",
    "    if slope > 0:\n",
    "        return \"aumento_significativo\"\n",
    "    elif slope < 0:\n",
    "        return \"reducao_significativa\"\n",
    "    else:\n",
    "        return \"sem_significancia\"\n",
    "\n",
    "df[\"classe_ensemble\"] = df.apply(classificar_ensemble, axis=1)\n",
    "\n",
    "# Ler shapefile minibacias\n",
    "minis_gdf = gpd.read_file(shp_minis)\n",
    "minis_gdf[ID_FIELD] = minis_gdf[ID_FIELD].astype(int)\n",
    "\n",
    "# Renomear colunas para Shapefile (<=10 caracteres)\n",
    "REN_SHAPE = {\n",
    "    \"ensemble_p\": \"ens_p\",\n",
    "    \"ensemble_slope\": \"ens_slope\",\n",
    "    \"ensemble_var_relativa_pct\": \"ens_var\",\n",
    "    \"classe_ensemble\": \"cls_ens\",\n",
    "}\n",
    "\n",
    "# Gerar shapefile por período\n",
    "for per in periodos_ordem:\n",
    "    df_p = df[df[\"periodo\"] == per].copy()\n",
    "\n",
    "    # Seleciona colunas essenciais\n",
    "    df_p = df_p[[\"sub_bacia\", \"ensemble_p\", \"ensemble_slope\", \"ensemble_var_relativa_pct\", \"classe_ensemble\"]].copy()\n",
    "\n",
    "    # Merge\n",
    "    gdf_p = minis_gdf.merge(df_p, left_on=ID_FIELD, right_on=\"sub_bacia\", how=\"left\")\n",
    "\n",
    "    # Renomear\n",
    "    gdf_p = gdf_p.rename(columns=REN_SHAPE)\n",
    "\n",
    "    per_limpo = periodo_para_nome(per)\n",
    "    shp_out = shapes_dir / f\"minis_enssig_{per_limpo}.shp\"\n",
    "    gdf_p.to_file(shp_out)\n",
    "\n",
    "    print(f\"Shapefile salvo: {shp_out.name}\")\n",
    "\n",
    "print(\"\\n✔ ETAPA 1 concluída: shapefiles gerados em:\", shapes_dir)\n",
    "\n",
    "# ===================================================\n",
    "# 2. ETAPA 2 – GERAR MAPA 2×2 (4 PAINÉIS)\n",
    "# ===================================================\n",
    "\n",
    "print(\"\\n=== ETAPA 2: Gerando mapa 4 painéis (significância do ensemble) ===\\n\")\n",
    "\n",
    "# Caminhos dos shapefiles por período\n",
    "shps = {}\n",
    "for per in periodos_ordem:\n",
    "    per_limpo = periodo_para_nome(per)\n",
    "    shp_path = shapes_dir / f\"minis_enssig_{per_limpo}.shp\"\n",
    "    if not shp_path.exists():\n",
    "        raise FileNotFoundError(f\"Não encontrei shapefile do período {per}: {shp_path}\")\n",
    "    shps[per] = shp_path\n",
    "\n",
    "gdfs = {per: gpd.read_file(path) for per, path in shps.items()}\n",
    "\n",
    "CAMPO_CLASSE = \"cls_ens\"\n",
    "if CAMPO_CLASSE not in gdfs[periodos_ordem[0]].columns:\n",
    "    raise KeyError(\n",
    "        f\"Campo '{CAMPO_CLASSE}' não encontrado no shapefile.\\n\"\n",
    "        f\"Colunas: {list(gdfs[periodos_ordem[0]].columns)}\"\n",
    "    )\n",
    "\n",
    "crs_minis = gdfs[periodos_ordem[0]].crs\n",
    "\n",
    "# Sub-bacias e cidades\n",
    "gdf_sub = gpd.read_file(shp_sub).to_crs(crs_minis)\n",
    "gdf_cid = gpd.read_file(shp_cidades).to_crs(crs_minis)\n",
    "\n",
    "mask_cur = gdf_cid[CAMPO_NOME_CIDADE].str.contains(r\"^curitiba$\", case=False, na=False, regex=True)\n",
    "mask_un  = gdf_cid[CAMPO_NOME_CIDADE].str.contains(\"uni[aã]o da vit\", case=False, na=False, regex=True)\n",
    "cidades_sel = gdf_cid[mask_cur | mask_un].copy()\n",
    "\n",
    "# Bounds\n",
    "xmin, ymin, xmax, ymax = gdfs[periodos_ordem[0]].total_bounds\n",
    "dx = (xmax - xmin) * 0.01\n",
    "dy = (ymax - ymin) * 0.01\n",
    "\n",
    "# Figura 2×2 + legenda\n",
    "fig = plt.figure(figsize=(14, 10), dpi=300)\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.22], wspace=0.02, hspace=0.05)\n",
    "\n",
    "ax_11 = fig.add_subplot(gs[0, 0])\n",
    "ax_12 = fig.add_subplot(gs[0, 1])\n",
    "ax_21 = fig.add_subplot(gs[1, 0])\n",
    "ax_22 = fig.add_subplot(gs[1, 1])\n",
    "ax_leg = fig.add_subplot(gs[:, 2])\n",
    "\n",
    "axes = [ax_11, ax_12, ax_21, ax_22]\n",
    "# Ordem lógica (continua igual)\n",
    "painel_ordem = [\"Curto\", \"Médio\", \"Longo\", \"Total\"]\n",
    "\n",
    "# Rótulos de exibição\n",
    "rotulos_paineis = {\n",
    "    \"Curto\": \"Horizonte 2015–2040\",\n",
    "    \"Médio\": \"Horizonte 2041–2070\",\n",
    "    \"Longo\": \"Horizonte 2071–2100\",\n",
    "    \"Total\": \"Horizonte Total (2015–2100)\",\n",
    "}\n",
    "\n",
    "for ax, per in zip(axes, painel_ordem):\n",
    "    gdf = gdfs[per]\n",
    "\n",
    "    # Plot por classe\n",
    "    for classe, cor in CORES.items():\n",
    "        sel = gdf[gdf[CAMPO_CLASSE] == classe]\n",
    "        if len(sel) == 0:\n",
    "            continue\n",
    "        sel.plot(ax=ax, color=cor, edgecolor=\"black\", linewidth=0.05)\n",
    "\n",
    "    # Contorno sub-bacias\n",
    "    gdf_sub.boundary.plot(ax=ax, edgecolor=\"grey\", linewidth=1.2, zorder=3)\n",
    "\n",
    "    # Cidades + rótulos\n",
    "    if not cidades_sel.empty:\n",
    "        cidades_sel.plot(ax=ax, marker=\"^\", color=\"black\", markersize=35, zorder=4, linewidth=0)\n",
    "\n",
    "        for _, row in cidades_sel.iterrows():\n",
    "            x = row.geometry.x\n",
    "            y = row.geometry.y\n",
    "            nome = row[CAMPO_NOME_CIDADE]\n",
    "\n",
    "            txt = ax.text(\n",
    "                x + dx, y + dy, nome,\n",
    "                fontsize=8.5, color=\"white\",\n",
    "                ha=\"left\", va=\"bottom\", zorder=5\n",
    "            )\n",
    "            txt.set_path_effects([\n",
    "                patheffects.Stroke(linewidth=1.4, foreground=\"black\"),\n",
    "                patheffects.Normal()\n",
    "            ])\n",
    "\n",
    "    ax.set_title(rotulos_paineis[per], fontsize=11)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# 2.4 Legenda customizada (3 classes)\n",
    "ax_leg.axis(\"off\")\n",
    "\n",
    "# Parâmetros de layout\n",
    "n = len(LEGENDA)\n",
    "dy_leg = 0.08\n",
    "\n",
    "# Centro do bloco de itens\n",
    "y_center = 0.45\n",
    "y0 = y_center + (n - 1) * dy_leg / 2\n",
    "\n",
    "# TÍTULO DA LEGENDA (acima dos itens)\n",
    "ax_leg.text(\n",
    "    0.5, y0 + 0.08,\n",
    "    \"Ensemble (Significância)\",\n",
    "    transform=ax_leg.transAxes,\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\"\n",
    ")\n",
    "\n",
    "# ITENS DA LEGENDA\n",
    "for i, (key, label) in enumerate(LEGENDA):\n",
    "    y = y0 - i * dy_leg\n",
    "\n",
    "    # Caixa de cor\n",
    "    ax_leg.add_patch(\n",
    "        plt.Rectangle(\n",
    "            (0.18, y - 0.03),\n",
    "            0.12,\n",
    "            0.06,\n",
    "            color=CORES[key],\n",
    "            transform=ax_leg.transAxes,\n",
    "            ec=\"black\",\n",
    "            lw=0.4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Texto\n",
    "    ax_leg.text(\n",
    "        0.36, y,\n",
    "        label,\n",
    "        transform=ax_leg.transAxes,\n",
    "        fontsize=10,\n",
    "        va=\"center\",\n",
    "        ha=\"left\"\n",
    "    )\n",
    "\n",
    "# Título geral\n",
    "fig.suptitle(\n",
    "    \"Significância do ensemble – Vazão média anual (SSP2-4.5)\",\n",
    "    fontsize=14, weight=\"bold\", y=0.98\n",
    ")\n",
    "\n",
    "fig.tight_layout(rect=[0.01, 0.02, 0.98, 0.95])\n",
    "fig.savefig(fig_path, dpi=300)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\n✔ Figura salva: {fig_path}\")\n",
    "print(\"\\n✨ ETAPA 2 FINALIZADA – Mapa 4 painéis gerado com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b5339-7026-484f-8881-97cb98e4de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resumo geral da concordância multimodelo (por horizonte) + classes de consenso\n",
    "\n",
    "CORREÇÕES IMPORTANTES:\n",
    "1) Remove BOM (ex.: '\\ufeff') do cabeçalho do CSV, que fazia 'sub_bacia' virar '\\ufeffsub_bacia'.\n",
    "2) Contabiliza consenso por horizonte contando MINIBACIAS ÚNICAS (ID), não linhas.\n",
    "3) Usa as classes no padrão do seu arquivo:\n",
    "   - sem_consenso\n",
    "   - consenso_moderado\n",
    "   - consenso_forte\n",
    "\n",
    "Saídas (na pasta out_dir):\n",
    "- resumo_por_periodo_e_estatistica.csv\n",
    "- resumo_por_periodo_geral.csv\n",
    "- resumo_concordam_vs_nao_por_periodo.csv\n",
    "- resumo_texto_word.txt\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================================\n",
    "# CONFIGURAÇÕES\n",
    "# ==========================================================\n",
    "csv_path = Path(r\"E:\\RESULTADOS_AB2\\SSP5-85\\_concordancia\\concordancia_minibacias_ssp585.csv\")\n",
    "\n",
    "out_dir = Path(r\"E:\\RESULTADOS_AB2\\SSP5-85\\_concordancia\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ORDEM_PERIODOS = [\"Curto\", \"Médio\", \"Longo\", \"Total\"]\n",
    "\n",
    "# ID da minibacia (deixe assim; o script agora corrige BOM automaticamente)\n",
    "ID_MINIBACIA_COL = \"sub_bacia\"\n",
    "\n",
    "# Classes como aparecem no seu arquivo (print)\n",
    "CLASSES_ESPERADAS = [\"sem_consenso\", \"consenso_moderado\", \"consenso_forte\"]\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# FUNÇÕES\n",
    "# ==========================================================\n",
    "def limpar_nome_coluna(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove BOM e caracteres invisíveis do início/fim, padroniza para lowercase.\n",
    "    \"\"\"\n",
    "    if col is None:\n",
    "        return col\n",
    "    # remove BOM e caracteres de \"zero width\" comuns\n",
    "    return (\n",
    "        str(col)\n",
    "        .replace(\"\\ufeff\", \"\")\n",
    "        .replace(\"\\u200b\", \"\")\n",
    "        .strip()\n",
    "        .lower()\n",
    "    )\n",
    "\n",
    "\n",
    "def detectar_id_minibacia(df: pd.DataFrame) -> str:\n",
    "    candidatos = [\n",
    "        \"sub_bacia\", \"subbacia\", \"minibacia\", \"mini\", \"codigo_mini\", \"cod_mini\",\n",
    "        \"id_mini\", \"id_minibacia\", \"bacia_id\", \"id\"\n",
    "    ]\n",
    "    for c in candidatos:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(\n",
    "        \"Não encontrei uma coluna de ID da minibacia automaticamente.\\n\"\n",
    "        \"Defina ID_MINIBACIA_COL com o nome correto (ex.: 'sub_bacia' ou 'codigo_mini').\\n\"\n",
    "        f\"Colunas disponíveis: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normaliza_classe_consenso(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normaliza classe_consenso para:\n",
    "    - sem_consenso\n",
    "    - consenso_moderado\n",
    "    - consenso_forte\n",
    "    \"\"\"\n",
    "    x = (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .str.lower()\n",
    "         .str.replace(\" \", \"_\", regex=False)\n",
    "         .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "\n",
    "    # mapeamentos de variantes comuns\n",
    "    map_variantes = {\n",
    "        \"semconsenso\": \"sem_consenso\",\n",
    "        \"nao_consenso\": \"sem_consenso\",\n",
    "        \"não_consenso\": \"sem_consenso\",\n",
    "\n",
    "        \"moderado\": \"consenso_moderado\",\n",
    "        \"consenso__moderado\": \"consenso_moderado\",\n",
    "\n",
    "        \"forte\": \"consenso_forte\",\n",
    "        \"consenso__forte\": \"consenso_forte\",\n",
    "    }\n",
    "    return x.replace(map_variantes)\n",
    "\n",
    "\n",
    "def padroniza_colunas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # limpa BOM/invisíveis + lowercase\n",
    "    df.columns = [limpar_nome_coluna(c) for c in df.columns]\n",
    "\n",
    "    ren = {}\n",
    "\n",
    "    for cand in [\"periodo\", \"período\", \"horizonte\", \"horizon\"]:\n",
    "        if cand in df.columns:\n",
    "            ren[cand] = \"periodo\"\n",
    "            break\n",
    "\n",
    "    for cand in [\"estatistica\", \"estatística\", \"tipo\", \"variavel\", \"variável\", \"metric\"]:\n",
    "        if cand in df.columns:\n",
    "            ren[cand] = \"estatistica\"\n",
    "            break\n",
    "\n",
    "    for cand in [\"modelos_significativos\", \"modelos_significativo\", \"modelos_sig\", \"gcms_significativos\"]:\n",
    "        if cand in df.columns:\n",
    "            ren[cand] = \"modelos_significativos\"\n",
    "            break\n",
    "\n",
    "    if \"positivos\" in df.columns:\n",
    "        ren[\"positivos\"] = \"positivos\"\n",
    "    if \"negativos\" in df.columns:\n",
    "        ren[\"negativos\"] = \"negativos\"\n",
    "\n",
    "    for cand in [\"concordancia_pct\", \"concordância_pct\", \"concordancia\", \"concordância\", \"agreement_pct\"]:\n",
    "        if cand in df.columns:\n",
    "            ren[cand] = \"concordancia_pct\"\n",
    "            break\n",
    "\n",
    "    if \"classe_consenso\" in df.columns:\n",
    "        ren[\"classe_consenso\"] = \"classe_consenso\"\n",
    "\n",
    "    df = df.rename(columns=ren)\n",
    "\n",
    "    # se não existir estatistica, cria\n",
    "    if \"estatistica\" not in df.columns:\n",
    "        df[\"estatistica\"] = \"Geral\"\n",
    "\n",
    "    required = [\n",
    "        \"periodo\",\n",
    "        \"estatistica\",\n",
    "        \"modelos_significativos\",\n",
    "        \"positivos\",\n",
    "        \"negativos\",\n",
    "        \"concordancia_pct\",\n",
    "        \"classe_consenso\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Colunas obrigatórias ausentes no CSV: {missing}\\n\"\n",
    "            f\"Colunas encontradas: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # numéricos\n",
    "    for c in [\"modelos_significativos\", \"positivos\", \"negativos\", \"concordancia_pct\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # strings\n",
    "    df[\"periodo\"] = df[\"periodo\"].astype(str).str.strip()\n",
    "    df[\"estatistica\"] = df[\"estatistica\"].astype(str).str.strip()\n",
    "\n",
    "    # normaliza classe_consenso\n",
    "    df[\"classe_consenso\"] = normaliza_classe_consenso(df[\"classe_consenso\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resumo_por_grupo(df: pd.DataFrame, group_cols: list[str], id_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resumo por grupo contando MINIBACIAS ÚNICAS (id_col).\n",
    "    \"\"\"\n",
    "    g = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    out = g.agg(\n",
    "        modelos_sig_min=(\"modelos_significativos\", \"min\"),\n",
    "        modelos_sig_max=(\"modelos_significativos\", \"max\"),\n",
    "        conc_pct_min=(\"concordancia_pct\", \"min\"),\n",
    "        conc_pct_max=(\"concordancia_pct\", \"max\"),\n",
    "        n_minibacias=(id_col, \"nunique\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # minibacias com >=1 positivo/negativo (únicas)\n",
    "    pos = (\n",
    "        df.loc[df[\"positivos\"].fillna(0) > 0]\n",
    "          .groupby(group_cols, dropna=False)[id_col]\n",
    "          .nunique()\n",
    "          .rename(\"n_minibacias_pos\")\n",
    "          .reset_index()\n",
    "    )\n",
    "    neg = (\n",
    "        df.loc[df[\"negativos\"].fillna(0) > 0]\n",
    "          .groupby(group_cols, dropna=False)[id_col]\n",
    "          .nunique()\n",
    "          .rename(\"n_minibacias_neg\")\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    out = out.merge(pos, on=group_cols, how=\"left\").merge(neg, on=group_cols, how=\"left\")\n",
    "    out[\"n_minibacias_pos\"] = out[\"n_minibacias_pos\"].fillna(0).astype(int)\n",
    "    out[\"n_minibacias_neg\"] = out[\"n_minibacias_neg\"].fillna(0).astype(int)\n",
    "\n",
    "    # contagem por classe_consenso (únicas)\n",
    "    ctab = (\n",
    "        df.groupby(group_cols + [\"classe_consenso\"], dropna=False)[id_col]\n",
    "          .nunique()\n",
    "          .unstack(\"classe_consenso\", fill_value=0)\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    for c in CLASSES_ESPERADAS:\n",
    "        if c not in ctab.columns:\n",
    "            ctab[c] = 0\n",
    "\n",
    "    ctab = ctab.rename(\n",
    "        columns={\n",
    "            \"sem_consenso\": \"n_sem_consenso\",\n",
    "            \"consenso_moderado\": \"n_consenso_moderado\",\n",
    "            \"consenso_forte\": \"n_consenso_forte\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    out = out.merge(\n",
    "        ctab[group_cols + [\"n_sem_consenso\", \"n_consenso_moderado\", \"n_consenso_forte\"]],\n",
    "        on=group_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # concordam vs não\n",
    "    out[\"n_concordam\"] = out[\"n_consenso_moderado\"].fillna(0).astype(int) + out[\"n_consenso_forte\"].fillna(0).astype(int)\n",
    "    out[\"n_nao_concordam\"] = out[\"n_sem_consenso\"].fillna(0).astype(int)\n",
    "\n",
    "    # faixas como texto\n",
    "    out[\"faixa_modelos_significativos\"] = (\n",
    "        out[\"modelos_sig_min\"].astype(\"Int64\").astype(str) + \"–\" +\n",
    "        out[\"modelos_sig_max\"].astype(\"Int64\").astype(str)\n",
    "    )\n",
    "    out[\"faixa_concordancia_pct\"] = (\n",
    "        out[\"conc_pct_min\"].round(1).astype(str) + \"–\" +\n",
    "        out[\"conc_pct_max\"].round(1).astype(str)\n",
    "    )\n",
    "\n",
    "    front = group_cols + [\n",
    "        \"faixa_modelos_significativos\",\n",
    "        \"n_minibacias_pos\",\n",
    "        \"n_minibacias_neg\",\n",
    "        \"faixa_concordancia_pct\",\n",
    "        \"n_sem_consenso\",\n",
    "        \"n_consenso_moderado\",\n",
    "        \"n_consenso_forte\",\n",
    "        \"n_concordam\",\n",
    "        \"n_nao_concordam\",\n",
    "        \"n_minibacias\",\n",
    "        \"modelos_sig_min\", \"modelos_sig_max\",\n",
    "        \"conc_pct_min\", \"conc_pct_max\",\n",
    "    ]\n",
    "    return out[front]\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# LEITURA\n",
    "# ==========================================================\n",
    "df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "df = padroniza_colunas(df)\n",
    "\n",
    "# define coluna ID, com limpeza de BOM/invisíveis\n",
    "id_col_cfg = limpar_nome_coluna(ID_MINIBACIA_COL) if ID_MINIBACIA_COL else None\n",
    "if id_col_cfg:\n",
    "    if id_col_cfg not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"ID_MINIBACIA_COL='{ID_MINIBACIA_COL}' não existe no CSV após limpeza.\\n\"\n",
    "            f\"Colunas: {list(df.columns)}\"\n",
    "        )\n",
    "    id_col = id_col_cfg\n",
    "else:\n",
    "    id_col = detectar_id_minibacia(df)\n",
    "\n",
    "print(f\"[INFO] Coluna de ID da minibacia usada: {id_col}\")\n",
    "\n",
    "# ==========================================================\n",
    "# RESUMOS\n",
    "# ==========================================================\n",
    "res_periodo_est = resumo_por_grupo(df, [\"periodo\", \"estatistica\"], id_col=id_col)\n",
    "res_periodo_geral = resumo_por_grupo(df, [\"periodo\"], id_col=id_col)\n",
    "\n",
    "# ordenar períodos\n",
    "res_periodo_est[\"periodo_ord\"] = pd.Categorical(res_periodo_est[\"periodo\"], categories=ORDEM_PERIODOS, ordered=True)\n",
    "res_periodo_est = res_periodo_est.sort_values([\"periodo_ord\", \"estatistica\"]).drop(columns=[\"periodo_ord\"])\n",
    "\n",
    "res_periodo_geral[\"periodo_ord\"] = pd.Categorical(res_periodo_geral[\"periodo\"], categories=ORDEM_PERIODOS, ordered=True)\n",
    "res_periodo_geral = res_periodo_geral.sort_values(\"periodo_ord\").drop(columns=[\"periodo_ord\"])\n",
    "\n",
    "# tabela extra: só concordam vs não\n",
    "res_concordam_vs_nao = res_periodo_geral[[\"periodo\", \"n_concordam\", \"n_nao_concordam\", \"n_minibacias\"]].copy()\n",
    "\n",
    "# ==========================================================\n",
    "# SALVAR\n",
    "# ==========================================================\n",
    "res_periodo_est.to_csv(out_dir / \"resumo_por_periodo_e_estatistica.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "res_periodo_geral.to_csv(out_dir / \"resumo_por_periodo_geral.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "res_concordam_vs_nao.to_csv(out_dir / \"resumo_concordam_vs_nao_por_periodo.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"OK! Tabelas geradas em:\", out_dir)\n",
    "\n",
    "# ==========================================================\n",
    "# TEXTO PRONTO PARA COLAR NO WORD\n",
    "# ==========================================================\n",
    "linhas = []\n",
    "for _, r in res_periodo_geral.iterrows():\n",
    "    linhas.append(\n",
    "        f\"No horizonte {str(r['periodo']).lower()}, os modelos significativos variaram de \"\n",
    "        f\"{r['modelos_sig_min']:.0f} a {r['modelos_sig_max']:.0f}; \"\n",
    "        f\"em {int(r['n_minibacias_pos'])} minibacias houve ao menos um modelo com tendência positiva \"\n",
    "        f\"e em {int(r['n_minibacias_neg'])} minibacias houve ao menos um modelo com tendência negativa. \"\n",
    "        f\"O percentual de concordância variou de {r['conc_pct_min']:.1f}% a {r['conc_pct_max']:.1f}%. \"\n",
    "        f\"Quanto ao consenso: {int(r['n_concordam'])} minibacias com consenso (moderado+forte) e \"\n",
    "        f\"{int(r['n_nao_concordam'])} minibacias sem consenso \"\n",
    "        f\"(forte={int(r['n_consenso_forte'])}, moderado={int(r['n_consenso_moderado'])}, sem={int(r['n_sem_consenso'])}).\"\n",
    "    )\n",
    "\n",
    "txt_path = out_dir / \"resumo_texto_word.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(linhas))\n",
    "\n",
    "print(\"Texto pronto salvo em:\", txt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
